import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init

"""DSC Usage: DepthwiseSeparableConv(in_channels, out_channels, 3, 1, 1)"""
class DepthwiseSeparableConv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1,padding=0, kernels_per_layer=1):
        super().__init__()
        self.depthwise = nn.Conv2d(
            in_channels,
            in_channels * kernels_per_layer,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            groups=in_channels,
        )
        self.pointwise = nn.Conv2d(in_channels * kernels_per_layer, out_channels, kernel_size=1)

    def forward(self, x):
        x = self.depthwise(x)
        x = self.pointwise(x)
        return x
"""SelfAttention Usage: """   
class SelfAttentionBlock(nn.Module):
    def __init__(self, in_ch):
        super().__init__()
        self.group_norm = nn.GroupNorm(32, in_ch)
        self.proj_q = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)
        self.proj_k = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)
        self.proj_v = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)
        self.proj = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)
        self.gamma = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        B, C, H, W = x.shape
        h = self.group_norm(x)
        q = self.proj_q(h) #X*Wq
        k = self.proj_k(h) #X*Wk
        v = self.proj_v(h) #X*Wv

        q = q.permute(0, 2, 3, 1).view(B, H * W, C)
        k = k.view(B, C, H * W)
        w = torch.bmm(q, k) * (int(C) ** (-0.5))#QKt/sqrt(dk)[1,4096,4096]
        assert list(w.shape) == [B, H * W, H * W]
        w = F.softmax(w, dim=-1)#softmax(QKt/sqrt(dk))

        v = v.permute(0, 2, 3, 1).view(B, H * W, C)
        h = torch.bmm(w, v) #softmax(QKt/sqrt(dk))*V=Attention(Q,K,V)=head
        assert list(h.shape) == [B, H * W, C]
        h = h.view(B, H, W, C).permute(0, 3, 1, 2)
        h = self.proj(h)
        out =  self.gamma+h + x #resnet
        return out
"""Transformer Encoder Usage:self.transformer=TransformerBottleneck(512,dim_feedforward=1024)"""       
class TransformerEncoder(nn.Module):
    def __init__(self, in_channels, num_heads=8, dim_feedforward=1024, num_layers=3):
        super(TransformerEncoder, self).__init__()
        
        # Patch embedding: Flatten spatial dimensions and map to reduced feature dimension
        self.patch_embed = nn.Conv2d(in_channels, in_channels // 2, kernel_size=1)
        
        # Transformer Encoder Layers
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=in_channels // 2, 
            nhead=num_heads, 
            dim_feedforward=dim_feedforward
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # Reshape back to original dimensions after Transformer processing
        self.unpatch_embed = nn.Conv2d(in_channels // 2, in_channels, kernel_size=1)
    
    def forward(self, x):
        # x: (B, C, H, W)
        batch_size, C, H, W = x.shape
        
        # Patch Embedding: Flatten the spatial dimensions
        x = self.patch_embed(x).view(batch_size, C // 2, -1).permute(2, 0, 1)  # (N, B, C)
        
        # Transformer Encoder
        x = self.transformer_encoder(x)  # (N, B, C)
        
        # Reshape back to (B, C, H, W)
        x = x.permute(1, 2, 0).view(batch_size, C // 2, H, W)
        x = self.unpatch_embed(x)
        
        return x
"""CBAM Usage:CBAM(planes=input_channel_num)"""
class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.shared_MLP = nn.Sequential(
            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),
            nn.ReLU(),
            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)
        )
        # self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)
        # self.relu1 = nn.ReLU()
        # self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)

        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out =self.shared_MLP(self.avg_pool(x))# self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
        max_out =self.shared_MLP(self.max_pool(x))# self.fc2(self.relu1(self.fc1(self.max_pool(x))))
        out = avg_out + max_out
        return self.sigmoid(out)

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()

        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'
        padding = 3 if kernel_size == 7 else 1

        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return self.sigmoid(x)

class CBAM(nn.Module):
    def __init__(self, planes):
        super(CBAM, self).__init__()
        self.ca = ChannelAttention(planes)
        self.sa = SpatialAttention()

    def forward(self, x):
        x = self.ca(x) * x
        x = self.sa(x) * x
        return x
"""CSA Usage:CSA(planes=input_channel_num)"""
class ChannelAttentionEnhancement(nn.Module):
    def __init__(self, in_planes, ratio=16):
        super(ChannelAttentionEnhancement, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
           
        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // 16, 1, bias=False),
                               nn.ReLU(),
                               nn.Conv2d(in_planes // 16, in_planes, 1, bias=False))
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc(self.avg_pool(x))
        max_out = self.fc(self.max_pool(x))
        out = avg_out + max_out
        return self.sigmoid(out)

class SpatialAttentionExtractor(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttentionExtractor, self).__init__()

        self.samconv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.samconv(x)
        return self.sigmoid(x)

class CSA(nn.Module):
    def __init__(self, planes):
        super(CSA, self).__init__()
        self.ca = ChannelAttentionEnhancement(planes)
        self.sa = SpatialAttentionExtractor()

    def forward(self, x):
        x = self.ca(x) * x
        x = self.sa(x) * x
        return x
"""SE Usage: SEAttention(channel=512, reduction=8)"""
class SEAttention(nn.Module):
    # 初始化SE模块，channel为通道数，reduction为降维比率
    def __init__(self, channel=512, reduction=16):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)  # 自适应平均池化层，将特征图的空间维度压缩为1x1
        self.fc = nn.Sequential(  # 定义两个全连接层作为激励操作，通过降维和升维调整通道重要性
            nn.Linear(channel, channel // reduction, bias=False),  # 降维，减少参数数量和计算量
            nn.ReLU(inplace=True),  # ReLU激活函数，引入非线性
            nn.Linear(channel // reduction, channel, bias=False),  # 升维，恢复到原始通道数
            nn.Sigmoid()  # Sigmoid激活函数，输出每个通道的重要性系数
        )

    # 权重初始化方法
    def init_weights(self):
        for m in self.modules():  # 遍历模块中的所有子模块
            if isinstance(m, nn.Conv2d):  # 对于卷积层
                init.kaiming_normal_(m.weight, mode='fan_out')  # 使用Kaiming初始化方法初始化权重
                if m.bias is not None:
                    init.constant_(m.bias, 0)  # 如果有偏置项，则初始化为0
            elif isinstance(m, nn.BatchNorm2d):  # 对于批归一化层
                init.constant_(m.weight, 1)  # 权重初始化为1
                init.constant_(m.bias, 0)  # 偏置初始化为0
            elif isinstance(m, nn.Linear):  # 对于全连接层
                init.normal_(m.weight, std=0.001)  # 权重使用正态分布初始化
                if m.bias is not None:
                    init.constant_(m.bias, 0)  # 偏置初始化为0

    # 前向传播方法
    def forward(self, x):
        b, c, _, _ = x.size()  # 获取输入x的批量大小b和通道数c
        y = self.avg_pool(x).view(b, c)  # 通过自适应平均池化层后，调整形状以匹配全连接层的输入
        y = self.fc(y).view(b, c, 1, 1)  # 通过全连接层计算通道重要性，调整形状以匹配原始特征图的形状
        return x * y.expand_as(x)  # 将通道重要性系数应用到原始特征图上，进行特征重新校准

# class WindPowerModel(nn.Module):
#     def __init__(self, site=5, d_model=768, nhead=8, num_layers=6, dim_feedforward=768, dropout=0.1):
#         super(WindPowerModel, self).__init__()
#         self.site = site
#         self.input_proj = nn.Linear(8 * site, d_model)  # 将输入特征转换为 d_model 维度
        
#         encoder_layers = nn.TransformerEncoderLayer(
#             d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout
#         )
#         self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)
        
#         self.output_proj = nn.Linear(d_model, site)  # 变换为 site 维度的输出

#     def forward(self, x):
#         b, _, _ = x.shape  # x: [b, 1, 8*site]
#         x = x.squeeze(1)  # [b, 8*site]
#         x = self.input_proj(x)  # [b, d_model]
#         x = x.unsqueeze(1)  # [b, 1, d_model] (添加序列维度)
#         x = self.transformer_encoder(x)  # [b, 1, d_model]
#         x = self.output_proj(x).squeeze(1)  # [b, site]
#         return x

class WindPowerModel(nn.Module):
    def __init__(self, site=5, d_model=768, nhead=8, num_layers=6, dim_feedforward=768, dropout=0.1, embed_dim=768):
        super(WindPowerModel, self).__init__()
        self.site = site
        
        # 站点嵌入 (10个站点)
        self.station_emb = nn.Embedding(10, embed_dim)
        
        # 时间嵌入
        self.month_emb = nn.Embedding(12, embed_dim)
        self.day_emb = nn.Embedding(31, embed_dim)
        self.hour_emb = nn.Embedding(24, embed_dim)
        self.minute_emb = nn.Embedding(60, embed_dim)

        # 输入特征变换
        self.input_proj = nn.Linear(8 * site, d_model)  # d_model 需要减去时间嵌入部分
        
        # Transformer 编码器
        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)
        
        # 输出投影
        self.output_proj = nn.Linear(d_model, site)  

    def forward(self, station, month, day, hour, minute, x):
        """
        station: [batch] (站点索引，0~9)
        month: [batch] (1~12)
        day: [batch] (1~31)
        hour: [batch] (0~23)
        minute: [batch] (0~59)
        x: [batch, 1, 8*site] (输入特征)
        """
        b, _, _ = x.shape
        x = x.squeeze(1)  # [batch, 8*site]
        
        # 获取时间嵌入
        station_emb = self.station_emb(station)  # [batch, embed_dim]
        month_emb = self.month_emb(month - 1)    # [batch, embed_dim]  (减1使索引从0开始)
        day_emb = self.day_emb(day - 1)          # [batch, embed_dim]
        hour_emb = self.hour_emb(hour)           # [batch, embed_dim]
        minute_emb = self.minute_emb(minute)     # [batch, embed_dim]

        # 合并嵌入
        time_features = station_emb
        
        # 变换输入特征
        x = self.input_proj(x)  

        # 拼接时间嵌入
        x = x+time_features.squeeze(1)
        
        # Transformer 处理
        x = self.transformer_encoder(x)  # [batch, 1, d_model]
        x = self.output_proj(x).squeeze(1)  # [batch, site]
        
        return x